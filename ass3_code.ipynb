{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "NgotE3p9hZKn",
      "metadata": {
        "id": "NgotE3p9hZKn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import cluster\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import normalize\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import pickle\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "GTZkbTFq6BBq",
      "metadata": {
        "id": "GTZkbTFq6BBq"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------functions for unsupervised evaluation----------------------------------\n",
        "def distance(points, point): # compute the distance from a point to other/another points\n",
        "    if len(points.shape) == 1:\n",
        "        return np.sqrt(sum((points - point) ** 2))\n",
        "    else:\n",
        "        return np.sqrt(np.sum((points - point) ** 2, axis = 1))  #Ouput: float or array of float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8e208434",
      "metadata": {
        "id": "8e208434"
      },
      "outputs": [],
      "source": [
        "# TODO 1: compute SSE\n",
        "#  Input: data - the 2d locations of data points\n",
        "#         r_clusters - the clusters obtained by some clustering algorithm\n",
        "#         r_centers - the centers of clusters\n",
        "#  HINT: (1) for frequently used function, like computing the distance between a point and other point(s), it is better to write it as an independent sub-function for multiple calling\n",
        "#        (2) utilize the nd-array's built-in functions/utilities to make your code concise and efficient\n",
        "#        (3) OVERALL COMMENT: r_cluster and r_labels contain the same information but in different form, you can use r_labels to replace r_cluster as the input, if you prefer it\n",
        "\n",
        "def compute_SSE(data, r_clusters, r_centers):\n",
        "  SSE=0\n",
        "  for i, cluster in enumerate(r_clusters):\n",
        "    for index in cluster:\n",
        "      SSE += distance(data[index], r_centers[i])**2\n",
        "  return SSE\n",
        "\n",
        "def compute_SSB(data, r_clusters, r_centers): # computer SSB\n",
        "    center_1 = np.mean(data, axis=0)\n",
        "    SSB = 0\n",
        "    for k in range(len(r_clusters)):\n",
        "        SSB += len(r_clusters[k]) * np.sum(distance(r_centers[k, :] ,center_1) ** 2)\n",
        "    return SSB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "up09JjEC6D70",
      "metadata": {
        "id": "up09JjEC6D70"
      },
      "outputs": [],
      "source": [
        "# TODO 2: compute the average silhouette coefficient (for all points)\n",
        "\n",
        "def nearest_cluster_dist(data, datapoint, r_clusters, cluster_index):\n",
        "  b = [[] for _ in range(len(r_clusters))]\n",
        "  for i, cluster in enumerate(r_clusters):\n",
        "    if i != cluster_index:\n",
        "      for point in cluster:\n",
        "          b[i].append(distance(data[point], datapoint))\n",
        "      b[i] = np.average(b[i])\n",
        "  b = list(filter(None, b))\n",
        "  return min(b)\n",
        "\n",
        "def compute_avg_silhouette_coefficient(data, r_clusters):\n",
        "    sc = []    \n",
        "\n",
        "    for i, cluster in enumerate(r_clusters):\n",
        "      for index in cluster:\n",
        "        a = []\n",
        "        datapoint = data[index]\n",
        "        for point in cluster:\n",
        "          a.append(distance(data[point], datapoint))\n",
        "        a_avg = sum(a)/(len(cluster)-1)\n",
        "        b = nearest_cluster_dist(data, datapoint, r_clusters, i)\n",
        "        sc.append((b - a_avg)/max(a_avg,b))\n",
        "    avg_sc = np.average(sc)\n",
        "    return avg_sc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "UPHBfcpe6F5F",
      "metadata": {
        "id": "UPHBfcpe6F5F"
      },
      "outputs": [],
      "source": [
        "#from numpy.matrixlib.defmatrix import N\n",
        "# TODO 3: compute the proximity matrix\n",
        "#   HINT: use the similarity matrix, instead of dissimilarity\n",
        "#         normalization is needed, i.e., the maximum value of similarity is 1\n",
        "\n",
        "def compute_proximity_matrix(data):\n",
        "  N = data.shape[0]\n",
        "  proximity = np.zeros((N,N))\n",
        "  dist=[]\n",
        "  for i in data:\n",
        "    dist.append(distance(data,i))\n",
        "  normalisation=np.max(dist)\n",
        "  \n",
        "  for index,value in enumerate(data):\n",
        "    for index_, item in enumerate(data):\n",
        "      proximity[index][index_] = (normalisation-distance(value,item))/normalisation\n",
        "\n",
        "  return proximity\n",
        "\n",
        "\n",
        "def compute_clustering_matrix(r_labels): # compute the clustering matrix\n",
        "    N = data.shape[0]\n",
        "    clustering = np.zeros((N, N))\n",
        "    for i in range(N):\n",
        "        for j in range(N):\n",
        "            if r_labels[i] == r_labels[j]:  # check whether two points belong to the same cluster\n",
        "                clustering[i,j] = 1\n",
        "    return clustering\n",
        "\n",
        "# TODO 4: compute the correlation between two matrices A and B\n",
        "def correlation(A, B):\n",
        "  mean_A = np.mean(A)\n",
        "  mean_B = np.mean(B)\n",
        "  a = []\n",
        "  b = []\n",
        "  ab = []\n",
        "  a_square = []\n",
        "  b_square = []\n",
        "  sum_ab = []\n",
        "  sum_a_square = []\n",
        "  sum_b_square = []\n",
        "\n",
        "  for i in range(len(A)):\n",
        "    a.append(A[i]-mean_A)\n",
        "    b.append(B[i]-mean_B)\n",
        "\n",
        "  ab = np.multiply(a,b)\n",
        "  a_square = np.square(a)\n",
        "  b_square = np.square(b)\n",
        "  sum_ab = np.sum(ab)\n",
        "  sum_a_square = np.sum(a_square)\n",
        "  sum_b_square = np.sum(b_square)\n",
        "  correlation = sum_ab / np.sqrt(sum_a_square*sum_b_square)\n",
        "  return correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a958884c",
      "metadata": {
        "id": "a958884c"
      },
      "outputs": [],
      "source": [
        "from re import I\n",
        "# ----------------------------------functions for supervised evaluation----------------------------------\n",
        "# ---------- classification-oriented evaluation ----------------\n",
        "# TODO 5: compute the precision and recall\n",
        "#  Input: labels - the known labels (ground truth)\n",
        "#         r_clusters - the clusters obtained by some clustering algorithm\n",
        "def compute_precision_and_recall(labels, r_clusters):\n",
        "  N = len(r_clusters)\n",
        "  matrix = np.zeros((N,N))\n",
        "  for n in range(N):\n",
        "    for index in r_clusters[n]:\n",
        "      matrix[n, labels[index]] += 1\n",
        "  precision = (matrix.T / np.sum(matrix, axis = 1)).T\n",
        "  recall = matrix / np.sum(matrix, axis=0)\n",
        "  return precision, recall\n",
        "\n",
        "def compute_purity(precision): # compute the purity\n",
        "    return np.sum(np.max(precision, axis=1))\n",
        "\n",
        "# TODO 6: compute the entropy\n",
        "#  HINT: the 0 element(s) in precision will not be considered for computing entropy\n",
        "def compute_entropy(precision, r_clusters):\n",
        "  #number of objects in cluster i \n",
        "  m_i = [len(item) for item in r_clusters]\n",
        "  #number of objects\n",
        "  m = sum(m_i)\n",
        "  entropy = 0\n",
        "\n",
        "  for index, value in enumerate(precision):\n",
        "      ei = 0\n",
        "      for i,val in enumerate(value):\n",
        "          if val != 0:\n",
        "              ei-=((val * np.log2(val)))\n",
        "      entropy += (m_i[index]/m) * ei\n",
        "  return entropy\n",
        "\n",
        "\n",
        "#---------- similarity-oriented evaluation ----------------\n",
        "# TODO 7: compute rand_statistic and Jaccard_coeff\n",
        "#  Input: labels - the known labels (ground truth)\n",
        "#         r_lables - the clustering result by some algorithm, to be evaluated\n",
        "def compute_binary_similarity(labels, r_labels):  \n",
        "  f = {}\n",
        "\n",
        "  for item_l, item_rl in list(zip(compute_clustering_matrix(labels), compute_clustering_matrix(r_labels))):\n",
        "    for l, r_l in list(zip(item_l, item_rl)):\n",
        "      f[f'f_{l}{r_l}']  =  f.get(f'f_{l}{r_l}', 0) + 1 \n",
        "\n",
        "  rand_statistic = (f.get('f_0.00.0',0)+f.get('f_1.01.0',0))/(f.get('f_0.00.0',0)+f.get('f_1.01.0', 0)+ f.get('f_0.01.0',0)+f.get('f_1.00.0',0))\n",
        "  Jaccard_coeff = (f.get('f_1.01.0',0))/(f.get('f_1.01.0',0)+ f.get('f_0.01.0',0)+f.get('f_1.00.0',0))\n",
        "  \n",
        "  return rand_statistic, Jaccard_coeff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "13bfd6d7",
      "metadata": {
        "id": "13bfd6d7"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------ other functions --------------------------------------\n",
        "def generate_sparse_data(n): #random sparse data points. Input: the number of points\n",
        "    data = []\n",
        "    for i in range(n):\n",
        "        data.append([np.random.randn(), np.random.randn()])  # the location of one point\n",
        "    data = np.array(data)\n",
        "    return data\n",
        "\n",
        "def labels_to_clusters(labels): # derive the clusters from labels. Example: if the labels are [0,0,1,1], then the clusters will be [[0,1], [2,3]] where the number is the index of data points\n",
        "    cluster_num = np.max(labels) + 1\n",
        "    clusters = []\n",
        "    for k in range(cluster_num):\n",
        "        clusters.append([])\n",
        "    for i in range(len(labels)):\n",
        "        clusters[labels[i]].append(i)\n",
        "    return clusters\n",
        "\n",
        "def plot_data(data): # plot the data points\n",
        "    fig = plt.figure(figsize=(7, 4.5))\n",
        "    ax  = fig.add_subplot(1, 1, 1)\n",
        "    ax.scatter(data[:,0], data[:,1], c='black')\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('y')\n",
        "\n",
        "# TODO 10 (optional): personalized your plot style\n",
        "def plot_clusters(data, r_clusters): # plot the data points with different colors for different clusters\n",
        "    fig = plt.figure(figsize=(7, 4.5))\n",
        "    ax  = fig.add_subplot(1, 1, 1)\n",
        "    for k in range(len(r_clusters)):\n",
        "        ax.scatter(data[r_clusters[k],0], data[r_clusters[k],1])\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('y')\n",
        "\n",
        "def plot_line(values, x_ticks, x_label, y_label): # plot the variation of values\n",
        "    fig = plt.figure(figsize=(7, 4.5))\n",
        "    ax  = fig.add_subplot(1, 1, 1)\n",
        "    ax.plot(values)\n",
        "    plt.xticks(range(len(values)), x_ticks)\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "\n",
        "# TODO 11 (optional): personalized your plot style\n",
        "def plot_matrix(matrix, axis_name): # plot a matrix\n",
        "    n = matrix.shape[0]\n",
        "    fig = plt.figure(figsize=(7, 4.5))\n",
        "    ax  = fig.add_subplot(1, 1, 1)\n",
        "    im = ax.imshow(matrix, cmap=\"Blues\",  vmin=0)   # you can change cmap to use different colors\n",
        "    tick_locator = ticker.MaxNLocator()\n",
        "    cb1 = plt.colorbar(im)\n",
        "    cb1.locator = tick_locator\n",
        "    cb1.update_ticks()\n",
        "    plt.xticks(range(n))\n",
        "    plt.yticks(range(n))\n",
        "    plt.xlabel(axis_name)\n",
        "    plt.ylabel(axis_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hu8a2XnOTnIo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Hu8a2XnOTnIo",
        "outputId": "7be777b6-e197-4152-918b-e1058e0a46a2"
      },
      "outputs": [],
      "source": [
        "#-------------------------------------------- The main process ------------------------------------------\n",
        "# TODO 8: generate two datasets, one is in random sparse structure while another distributed as specific overall shapes\n",
        "#  HINT: (1) use the tiny dataset to check the correctness of your functions above. You may also need to build other tiny datasets for further check\n",
        "#        (2) for random sparse data points: call function generate_sparse_data(n), and you can choose any parameter \"n\" (n >= 50) as you like\n",
        "#        (3) for data points distributed as different shapes: use functions in package sklearn.datasets, like datasets.make_circles(...), datasets.make_moons(...), datasets.make_blobs(...), etc. Choose parameters as you like (at least 50 points and 3 clusters)\n",
        "#------------------------------------- Step 1: generate different datasets -------------------------------------\n",
        "\n",
        "# ----- 1.1 tiny dataset (only for debugging) --------\n",
        "#data = np.array([[0,1],[0,2], [0,4], [0,5]])  # the location of 2d data points\n",
        "#labels = np.array([0,0,1,1])  # the known classification\n",
        "#n_list = [2] # the number of clusters\n",
        "\n",
        "# ----- 1.2 random sparse data points (only for unsupervised evaluation) --------\n",
        "data = generate_sparse_data(100)\n",
        "n_list = [2,4,5] # for testing the best number of clusters in unsupervised evaluation\n",
        "SSEs, ASCs  = [], [] #ditto\n",
        "\n",
        "# ----- 1.3 data points distributed as different shapes (for both supervised and unsupervised evaluations) --------\n",
        "data, labels = datasets.make_blobs(n_samples=100, n_features=2, centers=3, cluster_std=1.0, center_box=(-10.0, 10.0), shuffle=True, random_state=None)\n",
        "\n",
        "n_list = [2,3,4] # for testing the best number of clusters in unsupervised evaluation\n",
        "n_list = [3] # the fixed number of clusters in supervised evaluation\n",
        "\n",
        "plot_data(data)\n",
        "for n_cluster in n_list:\n",
        "    # ------------------------------------- Step 2: Clustering ------------------------------------------\n",
        "    # TODO 9: replace the kmeans algorithm by the DBSCAN algorithm (optional: you can also try other algorithms)\n",
        "    #  HINT: call function cluster.DBSCAN(...) (KMeans)\n",
        "    result = cluster.KMeans(n_clusters=n_cluster, random_state=0).fit(data) # call the kmeans algorithm\n",
        "    #result = cluster.DBSCAN(min_samples=10,eps=1).fit(data) # call the DBSCAN algorithm\n",
        "\n",
        "    r_labels = result.labels_  # the cluster labels for points\n",
        "    r_clusters = labels_to_clusters(r_labels) # the clusters\n",
        "    r_centers = result.cluster_centers_  # the centers of clusters\n",
        "    print('labels:', r_labels)\n",
        "    print('clusters:', r_clusters)\n",
        "    print('cluster centers:\\n', r_centers)\n",
        "    plot_clusters(data, r_clusters) # plot the data points\n",
        "    print('\\n')\n",
        "\n",
        "    #--------------------------------- Step 3: Unsupervised evaluation ---------------------------------\n",
        "    #---------- 3.1 evaluation with a given number of clusters -----------------\n",
        "    print('Part 1: Unsupervised evaluation')\n",
        "    SSE = compute_SSE(data, r_clusters, r_centers)\n",
        "    SSEs.append(SSE)\n",
        "    print('SSE:', np.round(SSE,2))\n",
        "    SSB = compute_SSB(data, r_clusters, r_centers)\n",
        "    print('SSB:', np.round(SSB,2))\n",
        "    average_silhouette_coefficient = compute_avg_silhouette_coefficient(data, r_clusters)\n",
        "    ASCs.append(average_silhouette_coefficient)\n",
        "    print('average silhouette coefficient:', np.round(average_silhouette_coefficient,2))\n",
        "    proximity_matrix = compute_proximity_matrix(data)\n",
        "    print('proximity matrix:\\n', np.round(proximity_matrix,2))\n",
        "    plot_matrix(proximity_matrix, 'Points')\n",
        "    clustering_matrix = compute_clustering_matrix(r_labels)\n",
        "    print('clustering matrix:\\n', np.round(clustering_matrix,2))\n",
        "    plot_matrix(clustering_matrix, 'Points')\n",
        "    corr = correlation(proximity_matrix, clustering_matrix)\n",
        "    print('corr:', np.round(corr,2))\n",
        "    print('\\n')\n",
        "    \n",
        "    #---------------------------------Step 4: Supervised evaluation -------------------------------\n",
        "    #---------- 4.1 classification-oriented evaluation ----------------\n",
        "    print('Part2: Supervised evaluation - classification-oriented')\n",
        "    precision, recall = compute_precision_and_recall(labels, r_clusters)\n",
        "    print('precision:\\n', np.round(precision,2))\n",
        "    print('recall:\\n', np.round(recall,2))\n",
        "    purity = compute_purity(precision)\n",
        "    print('purity:', np.round(purity,2))\n",
        "    entropy = compute_entropy(precision, r_clusters)\n",
        "    print('entropy:', np.round(entropy,2))\n",
        "    print('\\n')\n",
        "    \n",
        "    # ------------ 4.2 similarity-oriented evaluation ----------------\n",
        "    print('Part 3: Supervised evaluation - similarity-oriented')\n",
        "    rand_statistic, Jaccard_coeff = compute_binary_similarity(labels, r_labels)\n",
        "    print('rand_statistic:', np.round(rand_statistic,2))\n",
        "    print('Jaccard_coeff:', np.round(Jaccard_coeff,2))\n",
        "    print('\\n')\n",
        "    \n",
        "#------- 3.2 select the best number of clusters for unsupervised clustering -----------\n",
        "plot_line(SSEs, [str(n) for n in n_list], 'number of clusters', 'SSE')\n",
        "plot_line(ASCs, [str(n) for n in n_list], 'number of clusters', 'Average silhouette coefficient')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "40e6de3123e565ac040790e8da05e6aa545aae1be68c9c52db9c92ebebdebd36"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
